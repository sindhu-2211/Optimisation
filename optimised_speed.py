{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc91a37-6af6-431e-87f5-16c5bb417fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "\"\"\"\n",
    "Benchmark the inference speed of each module in LivePortrait - FIXED OPTIMIZED VERSION.\n",
    "Removes problematic torch.compile for Tesla T4 and focuses on proven optimizations.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "import yaml\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from src.utils.helper import load_model, concat_feat\n",
    "from src.config.inference_config import InferenceConfig\n",
    "\n",
    "\n",
    "def apply_optimizations():\n",
    "    \"\"\"Apply only proven PyTorch optimizations for Tesla T4\"\"\"\n",
    "    print(\"üöÄ Applying Tesla T4-optimized settings...\")\n",
    "    \n",
    "    # Core CUDA optimizations\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.backends.cudnn.deterministic = False\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        \n",
    "        device_props = torch.cuda.get_device_properties(0)\n",
    "        print(f\"‚úÖ GPU: {torch.cuda.get_device_name()}\")\n",
    "        print(f\"   Compute Capability: {device_props.major}.{device_props.minor}\")\n",
    "        print(f\"   Memory: {device_props.total_memory / 1e9:.1f}GB\")\n",
    "    \n",
    "    # Threading optimization\n",
    "    torch.set_num_threads(min(6, os.cpu_count() or 4))  # Reduced for Tesla T4\n",
    "    \n",
    "    # Environment variables for performance\n",
    "    os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"0\"\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = \"6\"\n",
    "    os.environ[\"MKL_NUM_THREADS\"] = \"6\"\n",
    "    \n",
    "    print(\"‚úÖ Tesla T4 optimizations applied\")\n",
    "\n",
    "\n",
    "def cleanup_memory():\n",
    "    \"\"\"Clean up GPU memory\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "def initialize_inputs(batch_size=1, device_id=0):\n",
    "    \"\"\"Generate random input tensors and move them to GPU\"\"\"\n",
    "    feature_3d = torch.randn(batch_size, 32, 16, 64, 64).to(device_id).half()\n",
    "    kp_source = torch.randn(batch_size, 21, 3).to(device_id).half()\n",
    "    kp_driving = torch.randn(batch_size, 21, 3).to(device_id).half()\n",
    "    source_image = torch.randn(batch_size, 3, 256, 256).to(device_id).half()\n",
    "    generator_input = torch.randn(batch_size, 256, 64, 64).to(device_id).half()\n",
    "    eye_close_ratio = torch.randn(batch_size, 3).to(device_id).half()\n",
    "    lip_close_ratio = torch.randn(batch_size, 2).to(device_id).half()\n",
    "    feat_stitching = concat_feat(kp_source, kp_driving).half()\n",
    "    feat_eye = concat_feat(kp_source, eye_close_ratio).half()\n",
    "    feat_lip = concat_feat(kp_source, lip_close_ratio).half()\n",
    "\n",
    "    inputs = {\n",
    "        'feature_3d': feature_3d,\n",
    "        'kp_source': kp_source,\n",
    "        'kp_driving': kp_driving,\n",
    "        'source_image': source_image,\n",
    "        'generator_input': generator_input,\n",
    "        'feat_stitching': feat_stitching,\n",
    "        'feat_eye': feat_eye,\n",
    "        'feat_lip': feat_lip\n",
    "    }\n",
    "\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def load_and_optimize_models_tesla_t4(cfg, model_config):\n",
    "    \"\"\"Load and optimize models specifically for Tesla T4\"\"\"\n",
    "    print(\"üì¶ Loading models with Tesla T4 optimizations...\")\n",
    "    \n",
    "    appearance_feature_extractor = load_model(cfg.checkpoint_F, model_config, cfg.device_id, 'appearance_feature_extractor')\n",
    "    motion_extractor = load_model(cfg.checkpoint_M, model_config, cfg.device_id, 'motion_extractor')\n",
    "    warping_module = load_model(cfg.checkpoint_W, model_config, cfg.device_id, 'warping_module')\n",
    "    spade_generator = load_model(cfg.checkpoint_G, model_config, cfg.device_id, 'spade_generator')\n",
    "    stitching_retargeting_module = load_model(cfg.checkpoint_S, model_config, cfg.device_id, 'stitching_retargeting_module')\n",
    "\n",
    "    models_with_params = [\n",
    "        ('Appearance Feature Extractor', appearance_feature_extractor),\n",
    "        ('Motion Extractor', motion_extractor),\n",
    "        ('Warping Network', warping_module),\n",
    "        ('SPADE Decoder', spade_generator)\n",
    "    ]\n",
    "\n",
    "    # Check GPU compute capability\n",
    "    compute_capability = torch.cuda.get_device_properties(0).major\n",
    "    use_compile = compute_capability >= 8  # Only use torch.compile on Ampere+ (RTX 30xx+)\n",
    "    \n",
    "    print(f\"   Compute Capability: {compute_capability}\")\n",
    "    print(f\"   Using torch.compile: {use_compile}\")\n",
    "\n",
    "    compiled_models = {}\n",
    "    for name, model in models_with_params:\n",
    "        model = model.half()\n",
    "        \n",
    "        if use_compile and hasattr(torch, 'compile'):\n",
    "            # Use less aggressive compilation for older GPUs\n",
    "            model = torch.compile(model, mode='reduce-overhead')\n",
    "        \n",
    "        model.eval()\n",
    "        compiled_models[name] = model\n",
    "\n",
    "    retargeting_models = ['stitching', 'eye', 'lip']\n",
    "    for retarget in retargeting_models:\n",
    "        module = stitching_retargeting_module[retarget].half()\n",
    "        \n",
    "        if use_compile and hasattr(torch, 'compile'):\n",
    "            module = torch.compile(module, mode='reduce-overhead')\n",
    "        \n",
    "        module.eval()\n",
    "        stitching_retargeting_module[retarget] = module\n",
    "\n",
    "    print(\"‚úÖ Models loaded and optimized for Tesla T4\")\n",
    "    return compiled_models, stitching_retargeting_module\n",
    "\n",
    "\n",
    "def warm_up_models_efficient(compiled_models, stitching_retargeting_module, inputs):\n",
    "    \"\"\"Efficient warm up for Tesla T4\"\"\"\n",
    "    print(\"üî• Efficient warm up for Tesla T4...\")\n",
    "    cleanup_memory()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Moderate warm-up iterations\n",
    "        for i in range(10):  # Reduced from 20\n",
    "            if i % 5 == 0:\n",
    "                print(f\"   Warm-up iteration {i+1}/10\")\n",
    "            \n",
    "            compiled_models['Appearance Feature Extractor'](inputs['source_image'])\n",
    "            compiled_models['Motion Extractor'](inputs['source_image'])\n",
    "            compiled_models['Warping Network'](inputs['feature_3d'], inputs['kp_driving'], inputs['kp_source'])\n",
    "            compiled_models['SPADE Decoder'](inputs['generator_input'])\n",
    "            stitching_retargeting_module['stitching'](inputs['feat_stitching'])\n",
    "            stitching_retargeting_module['eye'](inputs['feat_eye'])\n",
    "            stitching_retargeting_module['lip'](inputs['feat_lip'])\n",
    "    \n",
    "    print(\"‚úÖ Efficient warm up completed\")\n",
    "\n",
    "\n",
    "def measure_inference_times_optimized(compiled_models, stitching_retargeting_module, inputs, num_runs=100):\n",
    "    \"\"\"Measure inference times with Tesla T4 optimizations\"\"\"\n",
    "    print(f\"üìä Measuring inference times over {num_runs} runs...\")\n",
    "    \n",
    "    times = {name: [] for name in compiled_models.keys()}\n",
    "    times['Stitching and Retargeting Modules'] = []\n",
    "    overall_times = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_runs):\n",
    "            if i % 25 == 0:\n",
    "                print(f\"   Progress: {i+1}/{num_runs}\")\n",
    "            \n",
    "            # Overall timing\n",
    "            torch.cuda.synchronize()\n",
    "            overall_start = time.time()\n",
    "\n",
    "            # Appearance Feature Extractor\n",
    "            torch.cuda.synchronize()\n",
    "            start = time.time()\n",
    "            compiled_models['Appearance Feature Extractor'](inputs['source_image'])\n",
    "            torch.cuda.synchronize()\n",
    "            times['Appearance Feature Extractor'].append(time.time() - start)\n",
    "\n",
    "            # Motion Extractor\n",
    "            torch.cuda.synchronize()\n",
    "            start = time.time()\n",
    "            compiled_models['Motion Extractor'](inputs['source_image'])\n",
    "            torch.cuda.synchronize()\n",
    "            times['Motion Extractor'].append(time.time() - start)\n",
    "\n",
    "            # Warping Network\n",
    "            torch.cuda.synchronize()\n",
    "            start = time.time()\n",
    "            compiled_models['Warping Network'](inputs['feature_3d'], inputs['kp_driving'], inputs['kp_source'])\n",
    "            torch.cuda.synchronize()\n",
    "            times['Warping Network'].append(time.time() - start)\n",
    "\n",
    "            # SPADE Decoder\n",
    "            torch.cuda.synchronize()\n",
    "            start = time.time()\n",
    "            compiled_models['SPADE Decoder'](inputs['generator_input'])\n",
    "            torch.cuda.synchronize()\n",
    "            times['SPADE Decoder'].append(time.time() - start)\n",
    "\n",
    "            # Stitching and Retargeting\n",
    "            torch.cuda.synchronize()\n",
    "            start = time.time()\n",
    "            stitching_retargeting_module['stitching'](inputs['feat_stitching'])\n",
    "            stitching_retargeting_module['eye'](inputs['feat_eye'])\n",
    "            stitching_retargeting_module['lip'](inputs['feat_lip'])\n",
    "            torch.cuda.synchronize()\n",
    "            times['Stitching and Retargeting Modules'].append(time.time() - start)\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "            overall_times.append(time.time() - overall_start)\n",
    "\n",
    "    return times, overall_times\n",
    "\n",
    "\n",
    "def print_tesla_t4_benchmark_results(compiled_models, stitching_retargeting_module, retargeting_models, times, overall_times):\n",
    "    \"\"\"Print benchmark results optimized for Tesla T4\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üöÄ TESLA T4 OPTIMIZED LIVEPORTRAIT BENCHMARK RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    average_times = {name: np.mean(times[name]) * 1000 for name in times.keys()}\n",
    "    std_times = {name: np.std(times[name]) * 1000 for name in times.keys()}\n",
    "    min_times = {name: np.min(times[name]) * 1000 for name in times.keys()}\n",
    "    max_times = {name: np.max(times[name]) * 1000 for name in times.keys()}\n",
    "\n",
    "    # Model parameters\n",
    "    print(\"\\nüì¶ MODEL PARAMETERS:\")\n",
    "    total_params = 0\n",
    "    for name, model in compiled_models.items():\n",
    "        num_params = sum(p.numel() for p in model.parameters())\n",
    "        num_params_in_millions = num_params / 1e6\n",
    "        total_params += num_params\n",
    "        print(f\"   {name}: {num_params_in_millions:.2f}M parameters\")\n",
    "\n",
    "    for index, retarget in enumerate(retargeting_models):\n",
    "        num_params = sum(p.numel() for p in stitching_retargeting_module[retarget].parameters())\n",
    "        num_params_in_millions = num_params / 1e6\n",
    "        total_params += num_params\n",
    "        print(f\"   Retargeting {retarget}: {num_params_in_millions:.2f}M parameters\")\n",
    "    \n",
    "    print(f\"   TOTAL: {total_params/1e6:.2f}M parameters\")\n",
    "\n",
    "    # Timing results with comparison to original\n",
    "    print(f\"\\n‚è±Ô∏è  INFERENCE TIMES (averaged over {len(overall_times)} runs):\")\n",
    "    \n",
    "    # Reference times from original (for comparison)\n",
    "    original_times = {\n",
    "        'Appearance Feature Extractor': 6.41,\n",
    "        'Motion Extractor': 3.26,\n",
    "        'Warping Network': 38.21,\n",
    "        'SPADE Decoder': 41.27,\n",
    "        'Stitching and Retargeting Modules': 0.73\n",
    "    }\n",
    "    \n",
    "    for name in times.keys():\n",
    "        avg_time = average_times[name]\n",
    "        std_time = std_times[name]\n",
    "        min_time = min_times[name]\n",
    "        max_time = max_times[name]\n",
    "        \n",
    "        # Calculate improvement vs original\n",
    "        if name in original_times:\n",
    "            original_time = original_times[name]\n",
    "            improvement = ((original_time - avg_time) / original_time) * 100\n",
    "            if improvement > 0:\n",
    "                perf_indicator = f\"üü¢ {improvement:.1f}% faster\"\n",
    "            else:\n",
    "                perf_indicator = f\"üî¥ {abs(improvement):.1f}% slower\"\n",
    "        else:\n",
    "            perf_indicator = \"\"\n",
    "        \n",
    "        print(f\"   {name}:\")\n",
    "        print(f\"      Current: {avg_time:.2f}ms (¬±{std_time:.2f}ms) {perf_indicator}\")\n",
    "        print(f\"      Range: {min_time:.2f}ms - {max_time:.2f}ms\")\n",
    "\n",
    "    # Overall pipeline performance\n",
    "    overall_avg = np.mean(overall_times) * 1000\n",
    "    overall_std = np.std(overall_times) * 1000\n",
    "    overall_min = np.min(overall_times) * 1000\n",
    "    overall_max = np.max(overall_times) * 1000\n",
    "    fps = 1.0 / np.mean(overall_times)\n",
    "    \n",
    "    # Compare to original total\n",
    "    original_total = sum(original_times.values())\n",
    "    total_improvement = ((original_total - overall_avg) / original_total) * 100\n",
    "    \n",
    "    if total_improvement > 0:\n",
    "        total_perf = f\"üü¢ {total_improvement:.1f}% faster than original\"\n",
    "    else:\n",
    "        total_perf = f\"üî¥ {abs(total_improvement):.1f}% slower than original\"\n",
    "    \n",
    "    print(f\"\\nüéØ OVERALL PIPELINE PERFORMANCE:\")\n",
    "    print(f\"   Average: {overall_avg:.2f}ms (¬±{overall_std:.2f}ms)\")\n",
    "    print(f\"   Range: {overall_min:.2f}ms - {overall_max:.2f}ms\")\n",
    "    print(f\"   Estimated FPS: {fps:.1f}\")\n",
    "    print(f\"   vs Original: {total_perf}\")\n",
    "    \n",
    "    # GPU utilization info\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"\\nüñ•Ô∏è  GPU INFO:\")\n",
    "        print(f\"   Device: {torch.cuda.get_device_name()}\")\n",
    "        print(f\"   Memory Allocated: {torch.cuda.memory_allocated()/1e9:.2f}GB\")\n",
    "        print(f\"   Memory Reserved: {torch.cuda.memory_reserved()/1e9:.2f}GB\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to benchmark Tesla T4 optimized LivePortrait\"\"\"\n",
    "    print(\"üé≠ LivePortrait Tesla T4 Optimized Speed Benchmark\")\n",
    "    \n",
    "    # Apply Tesla T4 specific optimizations\n",
    "    apply_optimizations()\n",
    "    \n",
    "    # Load configuration\n",
    "    cfg = InferenceConfig()\n",
    "    model_config_path = cfg.models_config\n",
    "    with open(model_config_path, 'r') as file:\n",
    "        model_config = yaml.safe_load(file)\n",
    "\n",
    "    # Sample input tensors\n",
    "    inputs = initialize_inputs(device_id=cfg.device_id)\n",
    "\n",
    "    # Load and optimize models for Tesla T4\n",
    "    compiled_models, stitching_retargeting_module = load_and_optimize_models_tesla_t4(cfg, model_config)\n",
    "\n",
    "    # Efficient warm up\n",
    "    warm_up_models_efficient(compiled_models, stitching_retargeting_module, inputs)\n",
    "\n",
    "    # Measure inference times\n",
    "    times, overall_times = measure_inference_times_optimized(\n",
    "        compiled_models, stitching_retargeting_module, inputs, num_runs=100\n",
    "    )\n",
    "\n",
    "    # Print results with comparison\n",
    "    print_tesla_t4_benchmark_results(\n",
    "        compiled_models, stitching_retargeting_module, \n",
    "        ['stitching', 'eye', 'lip'], times, overall_times\n",
    "    )\n",
    "    \n",
    "    # Final cleanup\n",
    "    cleanup_memory()\n",
    "    print(\"\\n‚úÖ Tesla T4 optimized benchmark completed!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
