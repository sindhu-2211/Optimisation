{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca6ab46-a287-47a2-b1c3-8adfa3df6694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\"\"\"\n",
    "LivePortrait - Properly Optimized Version\n",
    "\"\"\"\n",
    "import os\n",
    "import os.path as osp\n",
    "import tyro\n",
    "import subprocess\n",
    "import torch\n",
    "import gc\n",
    "import time\n",
    "from src.config.argument_config import ArgumentConfig\n",
    "from src.config.inference_config import InferenceConfig\n",
    "from src.config.crop_config import CropConfig\n",
    "from src.live_portrait_pipeline import LivePortraitPipeline\n",
    "\n",
    "def apply_targeted_optimizations():\n",
    "    \"\"\"Apply targeted optimizations that actually help small models\"\"\"\n",
    "    print(\"üöÄ Applying targeted optimizations...\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        # Essential CUDA optimizations\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.backends.cudnn.deterministic = False\n",
    "        \n",
    "        # TF32 can help on newer GPUs (Ampere+), but may hurt on older ones like T4\n",
    "        # For Tesla T4, disable TF32 for better precision/performance balance\n",
    "        device_name = torch.cuda.get_device_name().lower()\n",
    "        if 't4' in device_name or 'v100' in device_name:\n",
    "            torch.backends.cuda.matmul.allow_tf32 = False\n",
    "            torch.backends.cudnn.allow_tf32 = False\n",
    "            print(\"‚úÖ TF32 disabled for older GPU\")\n",
    "        else:\n",
    "            torch.backends.cuda.matmul.allow_tf32 = True\n",
    "            torch.backends.cudnn.allow_tf32 = True\n",
    "            print(\"‚úÖ TF32 enabled for newer GPU\")\n",
    "        \n",
    "        # Enable mixed precision optimizations\n",
    "        torch.backends.cuda.enable_flash_sdp(True)\n",
    "        print(\"‚úÖ Flash Attention enabled\")\n",
    "        \n",
    "        print(f\"‚úÖ GPU: {torch.cuda.get_device_name()}\")\n",
    "    \n",
    "    # Conservative threading - too many threads can hurt small models\n",
    "    torch.set_num_threads(4)  # Fixed to 4, not dynamic\n",
    "    \n",
    "    # Minimal environment variables\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = \"4\"\n",
    "    os.environ[\"MKL_NUM_THREADS\"] = \"4\"\n",
    "    \n",
    "    print(\"‚úÖ Targeted optimizations applied\")\n",
    "\n",
    "def apply_mixed_precision(model, model_name):\n",
    "    \"\"\"Apply FP16 mixed precision to a model\"\"\"\n",
    "    try:\n",
    "        if torch.cuda.is_available():\n",
    "            model = model.half()  # Convert to FP16\n",
    "            print(f\"‚úÖ {model_name} converted to FP16\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è FP16 conversion failed for {model_name}: {e}\")\n",
    "        return model\n",
    "\n",
    "def optimize_pipeline_models(pipeline, compile_mode='reduce-overhead', use_fp16=True):\n",
    "    \"\"\"Apply model-specific optimizations with configurable settings\"\"\"\n",
    "    print(f\"üîß Optimizing pipeline models (mode: {compile_mode}, FP16: {use_fp16})...\")\n",
    "    \n",
    "    try:\n",
    "        # Skip very small models (< 1M params) - compilation overhead > benefits\n",
    "        small_models = ['appearance_feature_extractor', 'retargeting_stitching', \n",
    "                       'retargeting_eye', 'retargeting_lip']\n",
    "        \n",
    "        # Medium model - try light optimization\n",
    "        if hasattr(pipeline, 'motion_extractor'):\n",
    "            model = pipeline.motion_extractor\n",
    "            if use_fp16:\n",
    "                model = apply_mixed_precision(model, \"Motion Extractor\")\n",
    "                pipeline.motion_extractor = model\n",
    "            \n",
    "            # Only compile if it's significantly large (28M params)\n",
    "            if hasattr(torch, 'compile') and compile_mode != 'none':\n",
    "                pipeline.motion_extractor = torch.compile(\n",
    "                    pipeline.motion_extractor,\n",
    "                    mode='reduce-overhead',  # Conservative for medium model\n",
    "                    fullgraph=False\n",
    "                )\n",
    "                print(\"‚úÖ Motion Extractor optimized (light)\")\n",
    "        \n",
    "        # Large models - full optimization\n",
    "        if hasattr(pipeline, 'warping_network'):\n",
    "            model = pipeline.warping_network\n",
    "            if use_fp16:\n",
    "                model = apply_mixed_precision(model, \"Warping Network\")\n",
    "                pipeline.warping_network = model\n",
    "            \n",
    "            if hasattr(torch, 'compile') and compile_mode != 'none':\n",
    "                pipeline.warping_network = torch.compile(\n",
    "                    pipeline.warping_network, \n",
    "                    mode=compile_mode,\n",
    "                    fullgraph=False\n",
    "                )\n",
    "                print(f\"‚úÖ Warping Network optimized ({compile_mode})\")\n",
    "        \n",
    "        if hasattr(pipeline, 'spade_decoder'):\n",
    "            model = pipeline.spade_decoder\n",
    "            if use_fp16:\n",
    "                model = apply_mixed_precision(model, \"SPADE Decoder\")\n",
    "                pipeline.spade_decoder = model\n",
    "            \n",
    "            if hasattr(torch, 'compile') and compile_mode != 'none':\n",
    "                pipeline.spade_decoder = torch.compile(\n",
    "                    pipeline.spade_decoder,\n",
    "                    mode=compile_mode,\n",
    "                    fullgraph=False\n",
    "                )\n",
    "                print(f\"‚úÖ SPADE Decoder optimized ({compile_mode})\")\n",
    "        \n",
    "        print(f\"üéØ Model optimization completed\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Model optimization failed: {e}\")\n",
    "        print(\"üìã Continuing with standard models...\")\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "def partial_fields(target_class, kwargs):\n",
    "    return target_class(**{k: v for k, v in kwargs.items() if hasattr(target_class, k)})\n",
    "\n",
    "def fast_check_ffmpeg():\n",
    "    try:\n",
    "        subprocess.run([\"ffmpeg\", \"-version\"], capture_output=True, check=True)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def fast_check_args(args: ArgumentConfig):\n",
    "    args_dict = args.__dict__ if hasattr(args, '__dict__') else {}\n",
    "    \n",
    "    # Look for source file\n",
    "    source_path = None\n",
    "    for key in ['source', 's', 'src']:\n",
    "        if key in args_dict:\n",
    "            source_path = args_dict[key]\n",
    "            break\n",
    "    \n",
    "    # Look for driving file  \n",
    "    driving_path = None\n",
    "    for key in ['driving', 'd', 'drive']:\n",
    "        if key in args_dict:\n",
    "            driving_path = args_dict[key]\n",
    "            break\n",
    "    \n",
    "    # Validate files exist\n",
    "    if source_path and not osp.exists(source_path):\n",
    "        raise FileNotFoundError(f\"source info not found: {source_path}\")\n",
    "    if driving_path and not osp.exists(driving_path):\n",
    "        raise FileNotFoundError(f\"driving info not found: {driving_path}\")\n",
    "\n",
    "def cleanup_memory():\n",
    "    \"\"\"Efficient memory cleanup\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()  # Ensure cleanup completes\n",
    "    gc.collect()\n",
    "\n",
    "def main():\n",
    "    print(\"üé≠ LivePortrait - Advanced Optimized Version\")\n",
    "    \n",
    "    # Configuration for optimization experiments\n",
    "    COMPILE_MODE = 'max-autotune'  # Options: 'reduce-overhead', 'max-autotune', 'default', 'none'\n",
    "    USE_FP16 = True  # Enable/disable mixed precision\n",
    "    \n",
    "    print(f\"üîß Configuration: compile_mode={COMPILE_MODE}, fp16={USE_FP16}\")\n",
    "    \n",
    "    # Apply optimizations\n",
    "    apply_targeted_optimizations()\n",
    "    \n",
    "    # Original setup\n",
    "    tyro.extras.set_accent_color(\"bright_cyan\")\n",
    "    args = tyro.cli(ArgumentConfig)\n",
    "    \n",
    "    ffmpeg_dir = os.path.join(os.getcwd(), \"ffmpeg\")\n",
    "    if osp.exists(ffmpeg_dir):\n",
    "        os.environ[\"PATH\"] += (os.pathsep + ffmpeg_dir)\n",
    "    \n",
    "    if not fast_check_ffmpeg():\n",
    "        raise ImportError(\n",
    "            \"FFmpeg is not installed. Please install FFmpeg before running.\"\n",
    "        )\n",
    "    \n",
    "    fast_check_args(args)\n",
    "    \n",
    "    # Specify configs for inference\n",
    "    inference_cfg = partial_fields(InferenceConfig, args.__dict__)\n",
    "    crop_cfg = partial_fields(CropConfig, args.__dict__)\n",
    "    \n",
    "    # Create pipeline\n",
    "    print(\"üì¶ Loading pipeline...\")\n",
    "    live_portrait_pipeline = LivePortraitPipeline(\n",
    "        inference_cfg=inference_cfg,\n",
    "        crop_cfg=crop_cfg\n",
    "    )\n",
    "    \n",
    "    # Apply advanced optimizations\n",
    "    live_portrait_pipeline = optimize_pipeline_models(\n",
    "        live_portrait_pipeline, \n",
    "        compile_mode=COMPILE_MODE,\n",
    "        use_fp16=USE_FP16\n",
    "    )\n",
    "    \n",
    "    # Enhanced warmup for FP16\n",
    "    print(\"üî• Warming up optimized models...\")\n",
    "    if torch.cuda.is_available():\n",
    "        # FP16 warmup\n",
    "        if USE_FP16:\n",
    "            dummy_fp16 = torch.randn(1024, 1024, dtype=torch.float16).cuda()\n",
    "            _ = dummy_fp16 @ dummy_fp16\n",
    "            del dummy_fp16\n",
    "        \n",
    "        # Standard warmup\n",
    "        dummy = torch.randn(1024, 1024).cuda()\n",
    "        _ = dummy @ dummy\n",
    "        del dummy\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"‚úÖ GPU warmed up\")\n",
    "    \n",
    "    print(\"üöÄ Starting advanced optimized inference...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Use autocast for mixed precision during inference\n",
    "        if USE_FP16 and torch.cuda.is_available():\n",
    "            with torch.cuda.amp.autocast():\n",
    "                live_portrait_pipeline.execute(args)\n",
    "        else:\n",
    "            live_portrait_pipeline.execute(args)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"‚úÖ Completed in {end_time - start_time:.2f} seconds\")\n",
    "        print(f\"üéØ Settings used: {COMPILE_MODE} compilation, FP16={USE_FP16}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        if USE_FP16:\n",
    "            print(\"üí° Try setting USE_FP16=False if you encounter precision issues\")\n",
    "        raise\n",
    "    finally:\n",
    "        cleanup_memory()\n",
    "        print(\"üßπ Memory cleaned up\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
